variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  GIT_SUBMODULE_STRATEGY: recursive
  IMAGE: urm.nvidia.com/sw-tensorrt-docker/tensorrt-llm:dev-triton23.08-trt9.1.0.1
  TRT_ROOT: /usr/local/tensorrt
  GPT2: /home/scratch.trt_llm_data/llm-models/gpt2
  OPT_125M: /home/scratch.trt_llm_data/llm-models/opt-125m
  LLAMA: /home/scratch.trt_llm_data/llm-models/llama-models/llama-7b-hf
  GPTJ: /home/scratch.trt_llm_data/llm-models/gpt-j-6b

default:
  before_script:
    - export LD_LIBRARY_PATH="$TRT_ROOT/lib/:$LD_LIBRARY_PATH"
    - echo $LD_LIBRARY_PATH

cache:
  paths:
    - .cache/pip

stages:
  - style-check
  - package
  - build
  - test-model

workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "push"
    - if: $CI_COMMIT_BRANCH == "main" && $CI_PIPELINE_SOURCE == "schedule"

style-check-job:
  stage: style-check
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install pre-commit
    - pre-commit run -a

package-job:
  stage: package
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR/../
    - rm -rf tensorrt_llm_backend
    - mv tekit_backend tensorrt_llm_backend
    - cp tensorrt_llm_backend/scripts/package_trt_llm_backend.sh package_trt_llm_backend.sh
    - bash package_trt_llm_backend.sh tensorrt_llm_backend.tar.gz tensorrt_llm_backend
    - 'curl --header "JOB-TOKEN: $CI_JOB_TOKEN" --upload-file tensorrt_llm_backend.tar.gz "${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/tensorrt_llm_backend/${CI_COMMIT_SHORT_SHA}/tensorrt_llm_backend.tar.gz"'
    - mv tensorrt_llm_backend tekit_backend

build-job:
  stage: build
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - python3 tensorrt_llm/scripts/build_wheel.py -a "80" --trt_root $TRT_ROOT
    - cd $CI_PROJECT_DIR/inflight_batcher_llm
    - bash scripts/build.sh
  artifacts:
    expire_in: 30 mins
    paths:
      - tensorrt_llm/build
      - tensorrt_llm/cpp/build
      - inflight_batcher_llm/build

test-gpt-model-job:
  stage: test-model
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - bash tests/build_model.sh gpt
    - bash tests/test.sh gpt tensorrt_llm/examples/gpt/trt_engine/gpt2/fp16/1-gpu/ ${GPT2} auto
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job

test-opt-model-job:
  stage: test-model
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - bash tests/build_model.sh opt
    - bash tests/test.sh opt tensorrt_llm/examples/opt/trt_engine/opt-125m/fp16/1-gpu/ ${OPT_125M} auto
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job

test-llama-model-job:
  stage: test-model
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - bash tests/build_model.sh llama
    - bash tests/test.sh llama tensorrt_llm/examples/llama/llama_outputs ${LLAMA} llama
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job

test-gptj-model-job:
  stage: test-model
  image: $IMAGE
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - bash tests/build_model.sh gptj
    - bash tests/test.sh gptj tensorrt_llm/examples/gptj/gpt_outputs ${GPTJ} auto
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job

test-gpt-ib-model-job:
  stage: test-model
  image: $IMAGE
  parallel:
    matrix:
    - MAX_TOKENS_IN_KV_CACHE: ["", "2048"]
      BATCH_SCHEDULER_POLICY: ["max_utilization", "guaranteed_completion"]
      MAX_NUM_SEQUENCES: ["", "4", "32"]
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - mkdir /opt/tritonserver/backends/inflight_batcher_llm
    - cp inflight_batcher_llm/build/libtriton_inflight_batcher_llm.so /opt/tritonserver/backends/inflight_batcher_llm
    - bash tests/build_model.sh gpt-ib
    - bash tests/test.sh gpt-ib tensorrt_llm/examples/gpt/trt_engine/gpt2-ib/fp16/1-gpu/ ${GPT2} auto ${MAX_TOKENS_IN_KV_CACHE} ${BATCH_SCHEDULER_POLICY} ${MAX_NUM_SEQUENCES}
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job

test-gpt-ib-streaming-model-job:
  stage: test-model
  image: $IMAGE
  parallel:
    matrix:
    - MAX_TOKENS_IN_KV_CACHE: ["", "2048"]
      BATCH_SCHEDULER_POLICY: ["max_utilization", "guaranteed_completion"]
      MAX_NUM_SEQUENCES: ["", "4", "32"]
  script:
    - cd $CI_PROJECT_DIR
    - pip3 install --extra-index-url https://pypi.ngc.nvidia.com tensorrt_llm/build/tensorrt_llm*.whl
    - mkdir /opt/tritonserver/backends/inflight_batcher_llm
    - cp inflight_batcher_llm/build/libtriton_inflight_batcher_llm.so /opt/tritonserver/backends/inflight_batcher_llm
    - bash tests/build_model.sh gpt-ib
    - bash tests/test.sh gpt-ib-streaming tensorrt_llm/examples/gpt/trt_engine/gpt2-ib/fp16/1-gpu/ ${GPT2} auto ${MAX_TOKENS_IN_KV_CACHE} ${BATCH_SCHEDULER_POLICY} ${MAX_NUM_SEQUENCES}
  artifacts:
    expire_in: 30 mins
  dependencies:
    - build-job
